{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('../processed_data/5_split_dataset/data.npz', allow_pickle=True)\n",
    "label = np.load('../processed_data/5_split_dataset/label.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([data[f'arr_{i}'].item()['six_sec'] for i in range(len(data))])\n",
    "X_30sec = np.stack([data[f'arr_{i}'].item()['thirty_sec'] for i in range(len(data))])\n",
    "label = label\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_k_fold_dataset(data_6sec, data_30sec, label, k, shrink_ratio=1.0, shuffle_train_data=True, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    numbers = np.arange(data_6sec.shape[0])\n",
    "    np.random.shuffle(numbers)\n",
    "    test_indices = np.array_split(numbers, k)\n",
    "    \n",
    "    train_folds, test_6sec_folds, test_30sec_folds = [], [], []\n",
    "    for i in range(k):\n",
    "        mask = np.zeros(data_6sec.shape[0], dtype=bool)\n",
    "        mask[test_indices[i]] = True\n",
    "        \n",
    "        X_train, y_train = data_6sec[~mask], label[~mask]\n",
    "        if shuffle_train_data:\n",
    "            shuffle_indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(shuffle_indices)\n",
    "            X_train, y_train = X_train[shuffle_indices], y_train[shuffle_indices]\n",
    "        \n",
    "        X_6sec_test, y_6sec_test = data_6sec[mask], label[mask]\n",
    "        X_30sec_test, y_30sec_test = data_30sec[mask], label[mask]\n",
    "        \n",
    "        train_folds.append((X_train[:int(shrink_ratio * len(X_train))], y_train[:int(shrink_ratio * len(y_train))]))\n",
    "        test_6sec_folds.append((X_6sec_test, y_6sec_test))\n",
    "        test_30sec_folds.append((X_30sec_test, y_30sec_test))\n",
    "    \n",
    "    if shrink_ratio != 1.0:\n",
    "        print(f'Warning: shrink_ratio is set to {shrink_ratio}, the dataset is shrinked.')\n",
    "        print(f'X_train=({train_folds[0][0].shape}, y_train=({train_folds[0][1].shape}), X_6sec_test=({test_30sec_folds[0][0].shape}, y_6sec_test=({test_30sec_folds[0][1].shape})')\n",
    "    return train_folds, test_6sec_folds, test_30sec_folds\n",
    "        \n",
    "train_folds, test_6sec_folds, test_30sec_folds = generate_k_fold_dataset(X, X_30sec, label, 10, shrink_ratio=1.0, shuffle_train_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def predict_and_mode(model, X_test, scaler, threshold=0.6):\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    results = model.predict(X_test_scaled.reshape(-1, X_test.shape[-1]))\n",
    "    results = results.reshape(X_test.shape[:-1])\n",
    "    final_results = stats.mode(results, axis=1)\n",
    "    return final_results.mode.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive(model, X_test, scaler):\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    results = model.predict(X_test_scaled.reshape(-1, X_test.shape[-1]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def aggregate_metrics(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred),\\\n",
    "        precision_score(y_true, y_pred, average='macro', zero_division=0.),\\\n",
    "        recall_score(y_true, y_pred, average='macro', zero_division=0.),\\\n",
    "        f1_score(y_true, y_pred, average='macro', zero_division=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def perform_experiment(model, train_folds, test_6sec_folds, test_30sec_folds, fold_idx,\n",
    "                       type_return='report'):\n",
    "    \n",
    "    if type_return not in ['report', 'metrics']:\n",
    "        raise ValueError(\"Invalid return_type. Must be either 'report' or 'metrics'.\")\n",
    "    \n",
    "    X_train, y_train = train_folds[fold_idx]\n",
    "    X_test, y_test = test_6sec_folds[fold_idx]\n",
    "    X_test_30sec, y_test_30sec = test_30sec_folds[fold_idx]\n",
    "\n",
    "    rand_perm = np.random.permutation(np.arange(X_test.shape[0]))\n",
    "    X_test, y_test = X_test[rand_perm], y_test[rand_perm]\n",
    "    X_test_30sec, y_test_30sec = X_test_30sec[rand_perm], y_test_30sec[rand_perm]\n",
    "    \n",
    "    y_train = np.repeat(y_train, X_train.shape[1])\n",
    "    X_train = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "    rand_perm = np.random.permutation(np.arange(X_train.shape[0]))\n",
    "    X_train, y_train = X_train[rand_perm], y_train[rand_perm]\n",
    "    \n",
    "    scalar = StandardScaler()\n",
    "    X_train = scalar.fit_transform(X_train)\n",
    "\n",
    "    # Change the model here to experiment with different models\n",
    "    # model = SVC(kernel='rbf', decision_function_shape='ovo', C=1, probability=True, random_state=RANDOM_STATE)\n",
    "    # model = MLPClassifier(hidden_layer_sizes=(128, 128, 10), max_iter=1000, random_state=RANDOM_STATE)\n",
    "    # model = RandomForestClassifier(max_features=20, max_depth=10)\n",
    "\n",
    "    # model = RandomForestClassifier(max_features=20, max_depth=10)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if type_return == 'report':\n",
    "        train_report = classification_report(y_train, model.predict(X_train))\n",
    "        mode_report = classification_report(y_test, predict_and_mode(model, X_test, scalar))\n",
    "        naive_report = classification_report(y_test_30sec, predict_naive(model, X_test_30sec, scalar)) \n",
    "        return train_report, mode_report, naive_report\n",
    "    else:\n",
    "        train_metrics = aggregate_metrics(y_train, model.predict(X_train))\n",
    "        mode_metrics = aggregate_metrics(y_test, predict_and_mode(model, X_test, scalar))\n",
    "        naive_metrics = aggregate_metrics(y_test_30sec, predict_naive(model, X_test_30sec, scalar))\n",
    "        return train_metrics, mode_metrics, naive_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def k_fold_experiment(model, train_folds, test_6sec_folds, test_30sec_folds):\n",
    "    metrics = []\n",
    "    for k in tqdm(range(len(train_folds))):\n",
    "        train_metrics, mode_metrics, naive_metrics = perform_experiment(model, train_folds, test_6sec_folds, test_30sec_folds, k, type_return='metrics')\n",
    "        # print(f\"Fold {k+1}:\")\n",
    "        # print(f\"Train metrics: acc={train_metrics[0]:.3f}, precision={train_metrics[1]:.3f}, recall={train_metrics[2]:.3f}, f1={train_metrics[3]:.3f}\")\n",
    "        # print(f\"Mode Strategy metrics: acc={mode_metrics[0]:.3f}, precision={mode_metrics[1]:.3f}, recall={mode_metrics[2]:.3f}, f1={mode_metrics[3]:.3f}\")\n",
    "        # print(f\"Naive Strategy metrics: acc={naive_metrics[0]:.3f}, precision={naive_metrics[1]:.3f}, recall={naive_metrics[2]:.3f}, f1={naive_metrics[3]:.3f}\")\n",
    "        metrics.append((train_metrics, mode_metrics, naive_metrics))\n",
    "    \n",
    "    avg_accs = {\n",
    "        \"train\": np.mean([m[0][0] for m in metrics], axis=0),\n",
    "        \"mode\": np.mean([m[1][0] for m in metrics], axis=0),\n",
    "        \"naive\": np.mean([m[2][0] for m in metrics], axis=0)\n",
    "    }\n",
    "    avg_precisions = {\n",
    "        \"train\": np.mean([m[0][1] for m in metrics], axis=0),\n",
    "        \"mode\": np.mean([m[1][1] for m in metrics], axis=0),\n",
    "        \"naive\": np.mean([m[2][1] for m in metrics], axis=0)\n",
    "    }\n",
    "    avg_recalls = {\n",
    "        \"train\": np.mean([m[0][2] for m in metrics], axis=0),\n",
    "        \"mode\": np.mean([m[1][2] for m in metrics], axis=0),\n",
    "        \"naive\": np.mean([m[2][2] for m in metrics], axis=0)\n",
    "    }\n",
    "    avg_f1s = {\n",
    "        \"train\": np.mean([m[0][3] for m in metrics], axis=0),\n",
    "        \"mode\": np.mean([m[1][3] for m in metrics], axis=0),\n",
    "        \"naive\": np.mean([m[2][3] for m in metrics], axis=0)\n",
    "    }\n",
    "    \n",
    "    print(f\"Average train metrics: acc={avg_accs['train']:.3f},\" +\\\n",
    "          f\"precision={avg_precisions['train']:.3f},\" +\\\n",
    "          f\"recall={avg_recalls['train']:.3f},\" +\\\n",
    "          f\"f1={avg_f1s['train']:.3f}\")\n",
    "    print(f\"Average Mode Strategy metrics: acc={avg_accs['mode']:.3f},\" +\\\n",
    "          f\"precision={avg_precisions['mode']:.3f},\" +\\\n",
    "          f\"recall={avg_recalls['mode']:.3f},\" +\\\n",
    "          f\"f1={avg_f1s['mode']:.3f}\")\n",
    "    print(f\"Average Naive Strategy metrics: acc={avg_accs['naive']:.3f},\" +\\\n",
    "          f\"precision={avg_precisions['naive']:.3f},\" +\\\n",
    "          f\"recall={avg_recalls['naive']:.3f},\" +\\\n",
    "          f\"f1={avg_f1s['naive']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:44<00:00,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train metrics: acc=0.953,precision=0.953,recall=0.953,f1=0.953\n",
      "Average Mode Strategy metrics: acc=0.780,precision=0.775,recall=0.771,f1=0.761\n",
      "Average Naive Strategy metrics: acc=0.763,precision=0.759,recall=0.754,f1=0.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different models here\n",
    "model = SVC(kernel='rbf', decision_function_shape='ovo', C=1, probability=True, random_state=RANDOM_STATE)\n",
    "k_fold_experiment(model, train_folds, test_6sec_folds, test_30sec_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
