{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load('../processed_data/complete_dataset/processed_data.pt')\n",
    "label = torch.load('../processed_data/complete_dataset/processed_label.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_toolbox.metrics import audio_dataset_split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "X_train, y_train, X_val, y_val,\\\n",
    "X_test, y_test = audio_dataset_split(data, label, train_val_test_ratio=(0.9, 0.05, 0.05), random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.view(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.view(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.view(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=0.9)\n",
    "\n",
    "train_scaled = scaler.fit_transform(X_train_flat)\n",
    "val_scaled = scaler.transform(X_val_flat)\n",
    "test_scaled = scaler.transform(X_test_flat)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "train_data = torch.tensor(pca.fit_transform(train_scaled), dtype=torch.float, device=device)\n",
    "val_data = torch.tensor(pca.transform(val_scaled), dtype=torch.float, device=device)\n",
    "test_data = torch.tensor(pca.transform(test_scaled), dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train = torch.tensor(lb.fit_transform(y_train), device=device)\n",
    "y_val = torch.tensor(lb.fit_transform(y_val), device=device)\n",
    "y_test = torch.tensor(lb.fit_transform(y_test), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "datasets = {\n",
    "    'train': TensorDataset(train_data, y_train),\n",
    "    'val': TensorDataset(val_data, y_val),\n",
    "    'test': TensorDataset(test_data, y_test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_toolbox.models import SimpleLinearModel\n",
    "\n",
    "input_size = train_data.size(1)\n",
    "output_size = 10\n",
    "batch_size = 32\n",
    "\n",
    "model = SimpleLinearModel(input_size, output_size, hidden_dim=64, dropout_prob=0.8, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch import nn\n",
    "from audio_toolbox.models import OneHotCrossEntropyLoss\n",
    "\n",
    "loss_fn = OneHotCrossEntropyLoss()\n",
    "learning_rate = 1e-3  # Adjust the learning rate as needed\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_toolbox.trainer import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer(datasets, model, loss_fn, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    'save': False,\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': batch_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 0001, Learning rate: 0.001000, Training loss: 6.77485, Val loss: 2.42025, Test loss: 2.42162, Epoch time: 0.07936\n",
      "INFO:root:Epoch 0002, Learning rate: 0.001000, Training loss: 2.34804, Val loss: 2.11801, Test loss: 2.13294, Epoch time: 0.06527\n",
      "INFO:root:Epoch 0003, Learning rate: 0.001000, Training loss: 2.03767, Val loss: 1.81332, Test loss: 1.80676, Epoch time: 0.06823\n",
      "INFO:root:Epoch 0004, Learning rate: 0.001000, Training loss: 1.73715, Val loss: 1.51563, Test loss: 1.50991, Epoch time: 0.06501\n",
      "INFO:root:Epoch 0005, Learning rate: 0.001000, Training loss: 1.40630, Val loss: 1.17744, Test loss: 1.16791, Epoch time: 0.06435\n",
      "INFO:root:Epoch 0006, Learning rate: 0.000500, Training loss: 1.14874, Val loss: 1.01450, Test loss: 0.99978, Epoch time: 0.06189\n",
      "INFO:root:Epoch 0007, Learning rate: 0.000500, Training loss: 0.97137, Val loss: 0.86608, Test loss: 0.85254, Epoch time: 0.06789\n",
      "INFO:root:Epoch 0008, Learning rate: 0.000500, Training loss: 0.79682, Val loss: 0.70574, Test loss: 0.70744, Epoch time: 0.06549\n",
      "INFO:root:Epoch 0009, Learning rate: 0.000500, Training loss: 0.66260, Val loss: 0.57219, Test loss: 0.58221, Epoch time: 0.05739\n",
      "INFO:root:Epoch 0010, Learning rate: 0.000500, Training loss: 0.54575, Val loss: 0.45631, Test loss: 0.46996, Epoch time: 0.06464\n",
      "INFO:root:Epoch 0011, Learning rate: 0.000250, Training loss: 0.44844, Val loss: 0.40372, Test loss: 0.40444, Epoch time: 0.06251\n",
      "INFO:root:Epoch 0012, Learning rate: 0.000250, Training loss: 0.41198, Val loss: 0.36575, Test loss: 0.35918, Epoch time: 0.06132\n",
      "INFO:root:Epoch 0013, Learning rate: 0.000250, Training loss: 0.34552, Val loss: 0.32244, Test loss: 0.32248, Epoch time: 0.07012\n",
      "INFO:root:Epoch 0014, Learning rate: 0.000250, Training loss: 0.30601, Val loss: 0.28069, Test loss: 0.27721, Epoch time: 0.06551\n",
      "INFO:root:Epoch 0015, Learning rate: 0.000250, Training loss: 0.27239, Val loss: 0.24721, Test loss: 0.24967, Epoch time: 0.06809\n",
      "INFO:root:Epoch 0016, Learning rate: 0.000125, Training loss: 0.24734, Val loss: 0.25237, Test loss: 0.23956, Epoch time: 0.06141\n",
      "INFO:root:Epoch 0017, Learning rate: 0.000125, Training loss: 0.23198, Val loss: 0.22171, Test loss: 0.21825, Epoch time: 0.06454\n",
      "INFO:root:Epoch 0018, Learning rate: 0.000125, Training loss: 0.21644, Val loss: 0.20674, Test loss: 0.20365, Epoch time: 0.06189\n",
      "INFO:root:Epoch 0019, Learning rate: 0.000125, Training loss: 0.20217, Val loss: 0.19153, Test loss: 0.19814, Epoch time: 0.06508\n",
      "INFO:root:Epoch 0020, Learning rate: 0.000125, Training loss: 0.20542, Val loss: 0.18135, Test loss: 0.18861, Epoch time: 0.06830\n",
      "INFO:root:Epoch 0021, Learning rate: 0.000063, Training loss: 0.17692, Val loss: 0.17741, Test loss: 0.18188, Epoch time: 0.06550\n",
      "INFO:root:Epoch 0022, Learning rate: 0.000063, Training loss: 0.17372, Val loss: 0.16806, Test loss: 0.17841, Epoch time: 0.06485\n",
      "INFO:root:Epoch 0023, Learning rate: 0.000063, Training loss: 0.16533, Val loss: 0.16025, Test loss: 0.16399, Epoch time: 0.06832\n",
      "INFO:root:Epoch 0024, Learning rate: 0.000063, Training loss: 0.16294, Val loss: 0.15791, Test loss: 0.16189, Epoch time: 0.05758\n",
      "INFO:root:Epoch 0025, Learning rate: 0.000063, Training loss: 0.15512, Val loss: 0.15025, Test loss: 0.15447, Epoch time: 0.06534\n",
      "INFO:root:Epoch 0026, Learning rate: 0.000031, Training loss: 0.14951, Val loss: 0.15512, Test loss: 0.15920, Epoch time: 0.06255\n",
      "INFO:root:Epoch 0027, Learning rate: 0.000031, Training loss: 0.14902, Val loss: 0.14631, Test loss: 0.15123, Epoch time: 0.06511\n",
      "INFO:root:Epoch 0028, Learning rate: 0.000031, Training loss: 0.14780, Val loss: 0.14600, Test loss: 0.14719, Epoch time: 0.05738\n",
      "INFO:root:Epoch 0029, Learning rate: 0.000031, Training loss: 0.16117, Val loss: 0.14142, Test loss: 0.14406, Epoch time: 0.06753\n",
      "INFO:root:Epoch 0030, Learning rate: 0.000031, Training loss: 0.14367, Val loss: 0.13838, Test loss: 0.14142, Epoch time: 0.06209\n",
      "INFO:root:Epoch 0031, Learning rate: 0.000016, Training loss: 0.13976, Val loss: 0.13753, Test loss: 0.14398, Epoch time: 0.06486\n",
      "INFO:root:Epoch 0032, Learning rate: 0.000016, Training loss: 0.13803, Val loss: 0.13613, Test loss: 0.13802, Epoch time: 0.06501\n",
      "INFO:root:Epoch 0033, Learning rate: 0.000016, Training loss: 0.13665, Val loss: 0.13402, Test loss: 0.13393, Epoch time: 0.06064\n",
      "INFO:root:Epoch 0034, Learning rate: 0.000016, Training loss: 0.13251, Val loss: 0.13208, Test loss: 0.13312, Epoch time: 0.06140\n",
      "INFO:root:Epoch 0035, Learning rate: 0.000016, Training loss: 0.13148, Val loss: 0.12948, Test loss: 0.13083, Epoch time: 0.06552\n",
      "INFO:root:Epoch 0036, Learning rate: 0.000008, Training loss: 0.13409, Val loss: 0.13572, Test loss: 0.13214, Epoch time: 0.06866\n",
      "INFO:root:Epoch 0037, Learning rate: 0.000008, Training loss: 0.13088, Val loss: 0.12955, Test loss: 0.13104, Epoch time: 0.06491\n",
      "INFO:root:Epoch 0038, Learning rate: 0.000008, Training loss: 0.13078, Val loss: 0.12789, Test loss: 0.12987, Epoch time: 0.06168\n",
      "INFO:root:Epoch 0039, Learning rate: 0.000008, Training loss: 0.12998, Val loss: 0.12748, Test loss: 0.13061, Epoch time: 0.06520\n",
      "INFO:root:Epoch 0040, Learning rate: 0.000008, Training loss: 0.13079, Val loss: 0.13208, Test loss: 0.13134, Epoch time: 0.06837\n",
      "INFO:root:Epoch 0041, Learning rate: 0.000004, Training loss: 0.13002, Val loss: 0.13048, Test loss: 0.12845, Epoch time: 0.06578\n",
      "INFO:root:Epoch 0042, Learning rate: 0.000004, Training loss: 0.12989, Val loss: 0.12694, Test loss: 0.12679, Epoch time: 0.06408\n",
      "INFO:root:Epoch 0043, Learning rate: 0.000004, Training loss: 0.12657, Val loss: 0.12620, Test loss: 0.12767, Epoch time: 0.06509\n",
      "INFO:root:Epoch 0044, Learning rate: 0.000004, Training loss: 0.12937, Val loss: 0.12649, Test loss: 0.12605, Epoch time: 0.06842\n",
      "INFO:root:Epoch 0045, Learning rate: 0.000004, Training loss: 0.12724, Val loss: 0.12891, Test loss: 0.12948, Epoch time: 0.06536\n",
      "INFO:root:Epoch 0046, Learning rate: 0.000002, Training loss: 0.12743, Val loss: 0.12946, Test loss: 0.12735, Epoch time: 0.06069\n",
      "INFO:root:Epoch 0047, Learning rate: 0.000002, Training loss: 0.12667, Val loss: 0.12550, Test loss: 0.12533, Epoch time: 0.05757\n",
      "INFO:root:Epoch 0048, Learning rate: 0.000002, Training loss: 0.12638, Val loss: 0.12932, Test loss: 0.12456, Epoch time: 0.06056\n",
      "INFO:root:Epoch 0049, Learning rate: 0.000002, Training loss: 0.12404, Val loss: 0.12817, Test loss: 0.12476, Epoch time: 0.05754\n",
      "INFO:root:Epoch 0050, Learning rate: 0.000002, Training loss: 0.12605, Val loss: 0.12427, Test loss: 0.12419, Epoch time: 0.06534\n",
      "INFO:root:Epoch 0051, Learning rate: 0.000001, Training loss: 0.12505, Val loss: 0.12341, Test loss: 0.12303, Epoch time: 0.06189\n",
      "INFO:root:Epoch 0052, Learning rate: 0.000001, Training loss: 0.12544, Val loss: 0.12689, Test loss: 0.12435, Epoch time: 0.06455\n",
      "INFO:root:Epoch 0053, Learning rate: 0.000001, Training loss: 0.12488, Val loss: 0.12442, Test loss: 0.12681, Epoch time: 0.06124\n",
      "INFO:root:Epoch 0054, Learning rate: 0.000001, Training loss: 0.12514, Val loss: 0.12435, Test loss: 0.12390, Epoch time: 0.06537\n",
      "INFO:root:Epoch 0055, Learning rate: 0.000001, Training loss: 0.12670, Val loss: 0.12940, Test loss: 0.12254, Epoch time: 0.06544\n",
      "INFO:root:Epoch 0056, Learning rate: 0.000000, Training loss: 0.12352, Val loss: 0.12634, Test loss: 0.12675, Epoch time: 0.06494\n",
      "INFO:root:Epoch 0057, Learning rate: 0.000000, Training loss: 0.12712, Val loss: 0.12365, Test loss: 0.12451, Epoch time: 0.06863\n",
      "INFO:root:Epoch 0058, Learning rate: 0.000000, Training loss: 0.12734, Val loss: 0.12659, Test loss: 0.12322, Epoch time: 0.06593\n",
      "INFO:root:Epoch 0059, Learning rate: 0.000000, Training loss: 0.12940, Val loss: 0.12601, Test loss: 0.12269, Epoch time: 0.06541\n",
      "INFO:root:Epoch 0060, Learning rate: 0.000000, Training loss: 0.12690, Val loss: 0.12389, Test loss: 0.12292, Epoch time: 0.06819\n",
      "INFO:root:Epoch 0061, Learning rate: 0.000000, Training loss: 0.14050, Val loss: 0.12414, Test loss: 0.12271, Epoch time: 0.06549\n",
      "INFO:root:Epoch 0062, Learning rate: 0.000000, Training loss: 0.12535, Val loss: 0.12697, Test loss: 0.12298, Epoch time: 0.06110\n",
      "INFO:root:Epoch 0063, Learning rate: 0.000000, Training loss: 0.12562, Val loss: 0.12212, Test loss: 0.12373, Epoch time: 0.05750\n",
      "INFO:root:Epoch 0064, Learning rate: 0.000000, Training loss: 0.12610, Val loss: 0.12437, Test loss: 0.12394, Epoch time: 0.06366\n",
      "INFO:root:Epoch 0065, Learning rate: 0.000000, Training loss: 0.12379, Val loss: 0.12412, Test loss: 0.12558, Epoch time: 0.06223\n",
      "INFO:root:Epoch 0066, Learning rate: 0.000000, Training loss: 0.12301, Val loss: 0.12467, Test loss: 0.12642, Epoch time: 0.06889\n",
      "INFO:root:Epoch 0067, Learning rate: 0.000000, Training loss: 0.12519, Val loss: 0.12617, Test loss: 0.12743, Epoch time: 0.06960\n",
      "INFO:root:Epoch 0068, Learning rate: 0.000000, Training loss: 0.12391, Val loss: 0.12447, Test loss: 0.12364, Epoch time: 0.06538\n",
      "INFO:root:Epoch 0069, Learning rate: 0.000000, Training loss: 0.12522, Val loss: 0.12478, Test loss: 0.12610, Epoch time: 0.06529\n",
      "INFO:root:Epoch 0070, Learning rate: 0.000000, Training loss: 0.12735, Val loss: 0.12708, Test loss: 0.12518, Epoch time: 0.06881\n",
      "INFO:root:Epoch 0071, Learning rate: 0.000000, Training loss: 0.12363, Val loss: 0.12430, Test loss: 0.12640, Epoch time: 0.06518\n",
      "INFO:root:Epoch 0072, Learning rate: 0.000000, Training loss: 0.12393, Val loss: 0.12694, Test loss: 0.12367, Epoch time: 0.06521\n",
      "INFO:root:Epoch 0073, Learning rate: 0.000000, Training loss: 0.12262, Val loss: 0.12378, Test loss: 0.12423, Epoch time: 0.06539\n",
      "INFO:root:Epoch 0074, Learning rate: 0.000000, Training loss: 0.12385, Val loss: 0.12345, Test loss: 0.12909, Epoch time: 0.05798\n",
      "INFO:root:Epoch 0075, Learning rate: 0.000000, Training loss: 0.12822, Val loss: 0.12367, Test loss: 0.12398, Epoch time: 0.06776\n",
      "INFO:root:Epoch 0076, Learning rate: 0.000000, Training loss: 0.12392, Val loss: 0.12493, Test loss: 0.12368, Epoch time: 0.06618\n",
      "INFO:root:Epoch 0077, Learning rate: 0.000000, Training loss: 0.12317, Val loss: 0.12237, Test loss: 0.12510, Epoch time: 0.06719\n",
      "INFO:root:Epoch 0078, Learning rate: 0.000000, Training loss: 0.12429, Val loss: 0.13058, Test loss: 0.12495, Epoch time: 0.06973\n",
      "INFO:root:Epoch 0079, Learning rate: 0.000000, Training loss: 0.12395, Val loss: 0.12312, Test loss: 0.13280, Epoch time: 0.05830\n",
      "INFO:root:Epoch 0080, Learning rate: 0.000000, Training loss: 0.12525, Val loss: 0.13066, Test loss: 0.12548, Epoch time: 0.06213\n",
      "INFO:root:Epoch 0081, Learning rate: 0.000000, Training loss: 0.12422, Val loss: 0.12438, Test loss: 0.12366, Epoch time: 0.06507\n",
      "INFO:root:Epoch 0082, Learning rate: 0.000000, Training loss: 0.12466, Val loss: 0.12419, Test loss: 0.12684, Epoch time: 0.06519\n",
      "INFO:root:Epoch 0083, Learning rate: 0.000000, Training loss: 0.12348, Val loss: 0.12602, Test loss: 0.12424, Epoch time: 0.06481\n",
      "INFO:root:Epoch 0084, Learning rate: 0.000000, Training loss: 0.13499, Val loss: 0.12717, Test loss: 0.12703, Epoch time: 0.06354\n",
      "INFO:root:Epoch 0085, Learning rate: 0.000000, Training loss: 0.12329, Val loss: 0.12398, Test loss: 0.12334, Epoch time: 0.06167\n",
      "INFO:root:Epoch 0086, Learning rate: 0.000000, Training loss: 0.12662, Val loss: 0.12359, Test loss: 0.13048, Epoch time: 0.06214\n",
      "INFO:root:Epoch 0087, Learning rate: 0.000000, Training loss: 0.12257, Val loss: 0.12394, Test loss: 0.12955, Epoch time: 0.06042\n",
      "INFO:root:Epoch 0088, Learning rate: 0.000000, Training loss: 0.12753, Val loss: 0.12283, Test loss: 0.12420, Epoch time: 0.06526\n",
      "INFO:root:Epoch 0089, Learning rate: 0.000000, Training loss: 0.12785, Val loss: 0.12296, Test loss: 0.12327, Epoch time: 0.06534\n",
      "INFO:root:Epoch 0090, Learning rate: 0.000000, Training loss: 0.12330, Val loss: 0.12411, Test loss: 0.12466, Epoch time: 0.06518\n",
      "INFO:root:Epoch 0091, Learning rate: 0.000000, Training loss: 0.12253, Val loss: 0.12646, Test loss: 0.12486, Epoch time: 0.06839\n",
      "INFO:root:Epoch 0092, Learning rate: 0.000000, Training loss: 0.12579, Val loss: 0.12315, Test loss: 0.12941, Epoch time: 0.06596\n",
      "INFO:root:Epoch 0093, Learning rate: 0.000000, Training loss: 0.12459, Val loss: 0.12424, Test loss: 0.12738, Epoch time: 0.06981\n",
      "INFO:root:Epoch 0094, Learning rate: 0.000000, Training loss: 0.12370, Val loss: 0.12609, Test loss: 0.12601, Epoch time: 0.06496\n",
      "INFO:root:Epoch 0095, Learning rate: 0.000000, Training loss: 0.12675, Val loss: 0.12525, Test loss: 0.12847, Epoch time: 0.05755\n",
      "INFO:root:Epoch 0096, Learning rate: 0.000000, Training loss: 0.12235, Val loss: 0.12535, Test loss: 0.12448, Epoch time: 0.06909\n",
      "INFO:root:Epoch 0097, Learning rate: 0.000000, Training loss: 0.12412, Val loss: 0.12351, Test loss: 0.12266, Epoch time: 0.06610\n",
      "INFO:root:Epoch 0098, Learning rate: 0.000000, Training loss: 0.12599, Val loss: 0.12220, Test loss: 0.12372, Epoch time: 0.06546\n",
      "INFO:root:Epoch 0099, Learning rate: 0.000000, Training loss: 0.12662, Val loss: 0.12557, Test loss: 0.12431, Epoch time: 0.06894\n",
      "INFO:root:Epoch 0100, Learning rate: 0.000000, Training loss: 0.12488, Val loss: 0.12549, Test loss: 0.12333, Epoch time: 0.06549\n",
      "INFO:root:Best model at epoch 0063 with validation loss 0.12212\n"
     ]
    }
   ],
   "source": [
    "trainer.train(**trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = trainer.predict(train_data)\n",
    "val_res = trainer.predict(val_data)\n",
    "test_res = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_label = y_train.argmax(dim=1)\n",
    "y_val_label = y_val.argmax(dim=1)\n",
    "y_test_label = y_test.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.78%\n",
      "Validation accuracy: 46.94%\n",
      "Test accuracy: 52.94%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Train accuracy: {100 * accuracy_score(train_res.cpu(), y_train_label.cpu()):.2f}%\")\n",
    "print(f\"Validation accuracy: {100 * accuracy_score(val_res.cpu(), y_val_label.cpu()):.2f}%\")\n",
    "print(f\"Test accuracy: {100 * accuracy_score(test_res.cpu(), y_test_label.cpu()):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_toolbox.metrics import precision_recall\n",
    "\n",
    "_, _, _, f1_train = precision_recall(trainer, train_data, y_train_label)\n",
    "_, _, _, f1_val = precision_recall(trainer, val_data, y_val_label)\n",
    "_, _, _, f1_test = precision_recall(trainer, test_data, y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.9978\n",
      "Validation f1 score: 0.4342\n",
      "Test f1 score: 0.4468\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train f1 score: {f1_train:.4f}\")\n",
    "print(f\"Validation f1 score: {f1_val:.4f}\")\n",
    "print(f\"Test f1 score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
