{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.016230900Z",
     "start_time": "2024-04-03T16:03:45.919793400Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.append('..')\n",
    "audio_root_folder = '../archive/Data'\n",
    "labels_csv = os.path.join(audio_root_folder, 'data.csv')\n",
    "unslicedData = pd.read_csv(labels_csv, header=0)\n",
    "slicedCSV = os.path.join(audio_root_folder, 'features.csv')\n",
    "slicedDataSet = pd.read_csv(slicedCSV, header=0)\n",
    "label = unslicedData['label']\n",
    "unslicedData = unslicedData.sample(frac=1).reset_index(drop=True)\n",
    "pca = PCA(n_components=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def splitTabularPredicting(model, index, dataSet, scaler, pca):\n",
    "    \"\"\"\n",
    "    first find the sliced 10 data for each data in the testing data, then do the prediction to all 10 data.\n",
    "    Pick the mode of the prediction to be the final prediction, then compute the accuracy of this prediction.\n",
    "    Note that the first slice of filename \"blues.00000.wav\" is named as \"blues.00000.0.wav\"\n",
    "    Args:\n",
    "        pca: principal component analysis object\n",
    "        scaler: scaler used to scale data\n",
    "        index: the index columns of testing data\n",
    "        model: the model we trained\n",
    "        dataSet: The whole data set include the testing data\n",
    "    Returns:\n",
    "        result: the predicted label\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "    for i in range(len(index)):\n",
    "        songIndex = index.iloc[i]\n",
    "        sliceRow = dataSet[dataSet['index'] == songIndex].drop(['label', 'index'], axis=1)\n",
    "        sliceRow = pd.DataFrame(scaler.transform(sliceRow), columns=sliceRow.columns)\n",
    "        #sliceRow = pca.transform(sliceRow)\n",
    "        slice_prediction = model.predict(sliceRow)\n",
    "        prediction.append(Counter(slice_prediction).most_common(1)[0][0])\n",
    "    return np.array(prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.020656Z",
     "start_time": "2024-04-03T16:03:46.018233500Z"
    }
   },
   "id": "f4e8360938682304"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def naiveWeightTraining(model1, dataSet, scaler, model2):\n",
    "    \"\"\"\n",
    "    This method trains a weight function from the fitted model using the training data\n",
    "    Args:\n",
    "        model: the fitted model\n",
    "        dataSet: whole dataset\n",
    "        scaler: the scaler to scale the data. I prefer standardscaler\n",
    "    Returns:\n",
    "        newModel: the fitted weight function\n",
    "    \"\"\"\n",
    "    songSelected = dataSet.copy()\n",
    "    labels = songSelected['label']\n",
    "    songSelected.drop(['label', 'index'], axis=1, inplace=True)\n",
    "    scaledData = pd.DataFrame(scaler.transform(songSelected), columns=songSelected.columns)\n",
    "    rawPrediction = model1.predict(scaledData)\n",
    "    matchVector = np.where(labels == rawPrediction, 1, 0)\n",
    "    model2.fit(scaledData, matchVector)\n",
    "    return model2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.024880600Z",
     "start_time": "2024-04-03T16:03:46.022735700Z"
    }
   },
   "id": "dd23badcd6a51fb7"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def predictionUsingTwoModel(model1, model2, data, threshold):\n",
    "    \"\"\"\n",
    "    Use model1 to produce the sliced raw prediction for each slice.\n",
    "    Then for the song in the same index, use model2 to get the probability\n",
    "    Args:\n",
    "        threshould: \n",
    "        model1: \n",
    "        model2: \n",
    "        data: \n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    copy = data.copy()\n",
    "    index = copy['index']\n",
    "    copy.drop(['label', 'index'], axis=1, inplace=True)\n",
    "    rawPrediction = model1.predict(copy)\n",
    "    weight = model2.predict_proba(copy)[:, 1]\n",
    "    prediction_with_index = pd.DataFrame({\n",
    "        'index': index,\n",
    "        'probability': weight,\n",
    "        'prediction': rawPrediction\n",
    "    })\n",
    "    # Calculate the total probability for each index\n",
    "    total_probability = prediction_with_index.groupby('index')['probability'].transform('sum')\n",
    "    \n",
    "    # Normalize the probabilities\n",
    "    prediction_with_index['normalized_probability'] = prediction_with_index['probability'] / total_probability\n",
    "    \n",
    "    # Filter out the noise based on the threshold\n",
    "    filtered_predictions = prediction_with_index[prediction_with_index['normalized_probability'] >= threshold]\n",
    "    \n",
    "    # Continue with aggregation and selection of the highest probability prediction for each index\n",
    "    if not filtered_predictions.empty:\n",
    "        aggregated = filtered_predictions.groupby(['index', 'prediction'], as_index=False)['normalized_probability'].sum()\n",
    "        aggregated_sorted = aggregated.sort_values(by=['index', 'normalized_probability'], ascending=[True, False])\n",
    "        final_predictions = aggregated_sorted.drop_duplicates(subset=['index'], keep='first').sort_index()\n",
    "        final_predictions.drop(['normalized_probability'], axis=1, inplace=True)\n",
    "    else:\n",
    "        # Handle the case where filtering leaves some indices without predictions\n",
    "        final_predictions = pd.DataFrame(columns=['index', 'prediction'])\n",
    "    \n",
    "    return final_predictions\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.030250700Z",
     "start_time": "2024-04-03T16:03:46.024880600Z"
    }
   },
   "id": "94cb7bbe32df7979"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def getAccuracy(predictions, real):\n",
    "    \"\"\"\n",
    "    Return the accuracy of the prediction\n",
    "    Args:\n",
    "        real: the real label with index\n",
    "        predictions: the prediction with index\n",
    "    Returns:\n",
    "        the accuracy score\n",
    "    \"\"\"\n",
    "    merged = pd.merge(predictions, real, on='index')\n",
    "    correct_predictions = (merged['prediction'] == merged['label']).sum()\n",
    "    accuracy_score = correct_predictions / len(merged)\n",
    "    return accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.034212Z",
     "start_time": "2024-04-03T16:03:46.032335200Z"
    }
   },
   "id": "16ceff016afb66f9"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def predictionFinalStep(testData, model1, model2, scaler, threshold):\n",
    "    copy = testData.copy()\n",
    "    index = copy['index']\n",
    "    copy.drop(['label', 'index'], axis=1, inplace=True)\n",
    "    data_scaled = pd.DataFrame(scalar.fit_transform(copy), columns=copy.columns)\n",
    "    rawPrediction = model1.predict(data_scaled)\n",
    "    weight = model2.predict_proba(copy)[:, 1]\n",
    "    prediction_with_index = pd.DataFrame({\n",
    "        'index': index,\n",
    "        'probability': weight,\n",
    "        'prediction': rawPrediction\n",
    "    })\n",
    "    # Calculate the total probability for each index\n",
    "    total_probability = prediction_with_index.groupby('index')['probability'].transform('sum')\n",
    "    \n",
    "    # Normalize the probabilities\n",
    "    prediction_with_index['normalized_probability'] = prediction_with_index['probability'] / total_probability\n",
    "    \n",
    "    # Filter out the noise based on the threshold\n",
    "    filtered_predictions = prediction_with_index[prediction_with_index['normalized_probability'] >= threshold]\n",
    "    \n",
    "    # Continue with aggregation and selection of the highest probability prediction for each index\n",
    "    if not filtered_predictions.empty:\n",
    "        aggregated = filtered_predictions.groupby(['index', 'prediction'], as_index=False)['normalized_probability'].sum()\n",
    "        aggregated_sorted = aggregated.sort_values(by=['index', 'normalized_probability'], ascending=[True, False])\n",
    "        final_predictions = aggregated_sorted.drop_duplicates(subset=['index'], keep='first').sort_index()\n",
    "        final_predictions.drop(['normalized_probability'], axis=1, inplace=True)\n",
    "    else:\n",
    "        # Handle the case where filtering leaves some indices without predictions\n",
    "        final_predictions = pd.DataFrame(columns=['index', 'prediction'])\n",
    "    \n",
    "    return final_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.040028200Z",
     "start_time": "2024-04-03T16:03:46.037204Z"
    }
   },
   "id": "cfee54cdd00e9c17"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#scalar = StandardScaler()\n",
    "scalar = MinMaxScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.043619300Z",
     "start_time": "2024-04-03T16:03:46.040028200Z"
    }
   },
   "id": "602825d3fbc3d85"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "#generate the testing data folds\n",
    "K = 5\n",
    "testDataFolds = []\n",
    "testDataSliced = [] #the sliced testing data\n",
    "length = len(unslicedData)\n",
    "totalIndex = unslicedData['index']\n",
    "slice_size = length // K\n",
    "start_index = 0\n",
    "for i in range(K):\n",
    "    end_index = min(start_index + slice_size, length)\n",
    "    fold = unslicedData.iloc[start_index: end_index]\n",
    "    start_index = end_index\n",
    "    index = fold['index']\n",
    "    slicedTestData = slicedDataSet[slicedDataSet['index'].isin(index)]\n",
    "    testDataSliced.append(slicedTestData)\n",
    "    testDataFolds.append(fold)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.062639900Z",
     "start_time": "2024-04-03T16:03:46.042621200Z"
    }
   },
   "id": "87c452ac57bdc25d"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "#generate the training data folds\n",
    "trainData = []\n",
    "trainingLabel = []\n",
    "for i in range(K):\n",
    "    index = testDataFolds[i]['index']\n",
    "    label = unslicedData[~unslicedData['index'].isin(index)][['label', 'index']]\n",
    "    trainingLabel.append(label)\n",
    "    trainData.append(slicedDataSet[~slicedDataSet['index'].isin(index)])\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:03:46.070777100Z",
     "start_time": "2024-04-03T16:03:46.053663400Z"
    }
   },
   "id": "91127a6d4f7610c"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.53875\n",
      "training accuracy: 0.4775\n",
      "training accuracy: 0.51125\n",
      "training accuracy: 0.51375\n",
      "training accuracy: 0.5925\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "#model = RandomForestClassifier(max_depth = 15, max_features = 10)\n",
    "model = LogisticRegression(max_iter=10000, C=1e-3)\n",
    "#model = SVC(kernel='rbf', C=1)\n",
    "\n",
    "predictions = []\n",
    "testLabels = []\n",
    "threshold = 0.07\n",
    "for i in range(K):\n",
    "    X_train = trainData[i].copy()\n",
    "    index = X_train['index']\n",
    "    label = X_train['label']\n",
    "    X_train.drop(['label','index'], axis=1, inplace=True)\n",
    "    X_train_scaled = pd.DataFrame(scalar.fit_transform(X_train), columns=X_train.columns)\n",
    "    #X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    #X_train = pd.DataFrame(npArray, columns=X_train.columns)\n",
    "    #trainingData, _, trainingLabel, _ = train_test_split(X_train_pca, label, train_size=0.5)\n",
    "    #model.fit(trainingData, trainingLabel)\n",
    "    #model.fit(X_train_pca, label)\n",
    "    model.fit(X_train_scaled, label)\n",
    "    weightModel = RandomForestClassifier(max_depth=20, max_features=20)\n",
    "    weightModel = naiveWeightTraining(model, trainData[i], scalar, weightModel)\n",
    "    X_train_scaled['index'] = index.values\n",
    "    X_train_scaled['label'] =label.values\n",
    "    #model.fit(X_train, label)\n",
    "    #trainAccuracy = accuracy_score(model.predict(X_train), label)\n",
    "    trainPrediction = predictionUsingTwoModel(model, weightModel, X_train_scaled, threshold)\n",
    "    trainAccuracy = getAccuracy(trainPrediction, trainingLabel[i])\n",
    "    #trainAccuracy = accuracy_score(model.predict(X_train_pca), label)\n",
    "    print(f\"training accuracy: {trainAccuracy}\")\n",
    "    testLabel = testDataFolds[i][['label', 'index']] #index and label of 30 second songs together\n",
    "    testLabels.append(testLabel)\n",
    "    prediction = predictionFinalStep(testDataSliced[i], model, weightModel, scalar, threshold)\n",
    "    predictions.append(prediction)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:05:09.890127600Z",
     "start_time": "2024-04-03T16:03:46.073769400Z"
    }
   },
   "id": "f97cd40291c7b8ec"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.38994974874371857\n"
     ]
    }
   ],
   "source": [
    "#average accuracy\n",
    "accuracies = []\n",
    "for i in range(K):\n",
    "    prediction = predictions[i]\n",
    "    accuracyOfPrediction = getAccuracy(prediction, testLabels[i])\n",
    "    accuracies.append(accuracyOfPrediction)\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:05:09.896784900Z",
     "start_time": "2024-04-03T16:05:09.883147Z"
    }
   },
   "id": "a57d7e5e03d0e89a"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:05:09.896784900Z",
     "start_time": "2024-04-03T16:05:09.893354400Z"
    }
   },
   "id": "8c0dbd37e47b7862"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
