{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('../processed_data/split_dataset/data.npz', allow_pickle=True)\n",
    "label = np.load('../processed_data/split_dataset/label.npz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([data[f'arr_{i}'].item()['three_sec'] for i in range(len(data))])\n",
    "X_30sec = np.stack([data[f'arr_{i}'].item()['thirty_sec'] for i in range(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_k_fold_dataset(data_3sec, data_30sec, label, k):\n",
    "    numbers = np.arange(data_3sec.shape[0])\n",
    "    np.random.shuffle(numbers)\n",
    "    test_indices = np.array_split(numbers, k)\n",
    "    \n",
    "    train_folds, test_3sec_folds, test_30sec_folds = [], [], []\n",
    "    for i in range(k):\n",
    "        mask = np.zeros(data_3sec.shape[0], dtype=bool)\n",
    "        mask[test_indices[i]] = True\n",
    "        \n",
    "        X_train, y_train = data_3sec[~mask], label[~mask]\n",
    "        X_3sec_test, y_3sec_test = data_3sec[mask], label[mask]\n",
    "        X_30sec_test, y_30sec_test = data_30sec[mask], label[mask]\n",
    "        \n",
    "        train_folds.append((X_train, y_train))\n",
    "        test_3sec_folds.append((X_3sec_test, y_3sec_test))\n",
    "        test_30sec_folds.append((X_30sec_test, y_30sec_test))\n",
    "    \n",
    "    return train_folds, test_3sec_folds, test_30sec_folds\n",
    "        \n",
    "train_folds, test_3sec_folds, test_30sec_folds = generate_k_fold_dataset(X, X_30sec, label, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def predict_and_mode(model, X_test, scaler):\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    results = model.predict(X_test_scaled.reshape(-1, X_test.shape[-1]))\n",
    "    results = results.reshape(X_test.shape[:-1])\n",
    "    final_results = stats.mode(results, axis=1)\n",
    "\n",
    "    return final_results.mode.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive(model, X_test, scaler):\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    results = model.predict(X_test_scaled.reshape(-1, X_test.shape[-1]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def aggregate_metrics(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred),\\\n",
    "        precision_score(y_true, y_pred, average='macro', zero_division=0.),\\\n",
    "        recall_score(y_true, y_pred, average='macro', zero_division=0.),\\\n",
    "        f1_score(y_true, y_pred, average='macro', zero_division=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def perform_experiment(train_folds, test_3sec_folds, test_30sec_folds, fold_idx,\n",
    "                       type_return='report'):\n",
    "    \n",
    "    if type_return not in ['report', 'metrics']:\n",
    "        raise ValueError(\"Invalid return_type. Must be either 'report' or 'metrics'.\")\n",
    "    \n",
    "    X_train, y_train = train_folds[fold_idx]\n",
    "    X_test, y_test = test_3sec_folds[fold_idx]\n",
    "    X_test_30sec, y_test_30sec = test_30sec_folds[fold_idx]\n",
    "\n",
    "    rand_perm = np.random.permutation(np.arange(X_test.shape[0]))\n",
    "    X_test, y_test = X_test[rand_perm], y_test[rand_perm]\n",
    "    X_test_30sec, y_test_30sec = X_test_30sec[rand_perm], y_test_30sec[rand_perm]\n",
    "    \n",
    "    y_train = np.repeat(y_train, X_train.shape[1])\n",
    "    X_train = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "    rand_perm = np.random.permutation(np.arange(X_train.shape[0]))\n",
    "    X_train, y_train = X_train[rand_perm], y_train[rand_perm]\n",
    "    \n",
    "    scalar = StandardScaler()\n",
    "    X_train = scalar.fit_transform(X_train)\n",
    "\n",
    "    # Change the model here to experiment with different models\n",
    "    model = SVC(kernel='rbf', decision_function_shape='ovo', C=1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if type_return == 'report':\n",
    "        train_report = classification_report(y_train, model.predict(X_train))\n",
    "        mode_report = classification_report(y_test, predict_and_mode(model, X_test, scalar))\n",
    "        naive_report = classification_report(y_test_30sec, predict_naive(model, X_test_30sec, scalar)) \n",
    "        return train_report, mode_report, naive_report\n",
    "    else:\n",
    "        train_metrics = aggregate_metrics(y_train, model.predict(X_train))\n",
    "        mode_metrics = aggregate_metrics(y_test, predict_and_mode(model, X_test, scalar))\n",
    "        naive_metrics = aggregate_metrics(y_test_30sec, predict_naive(model, X_test_30sec, scalar))\n",
    "        return train_metrics, mode_metrics, naive_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_experiment(train_folds, test_3sec_folds, test_30sec_folds):\n",
    "    metrics = []\n",
    "    for k in range(len(train_folds)):\n",
    "        train_metrics, mode_metrics, naive_metrics = perform_experiment(train_folds, test_3sec_folds, test_30sec_folds, k, type_return='metrics')\n",
    "        print(f\"Fold {k+1}:\")\n",
    "        print(f\"Train metrics: acc={train_metrics[0]:.3f}, precision={train_metrics[1]:.3f}, recall={train_metrics[2]:.3f}, f1={train_metrics[3]:.3f}\")\n",
    "        print(f\"Mode Strategy metrics: acc={mode_metrics[0]:.3f}, precision={mode_metrics[1]:.3f}, recall={mode_metrics[2]:.3f}, f1={mode_metrics[3]:.3f}\")\n",
    "        print(f\"Naive Strategy metrics: acc={naive_metrics[0]:.3f}, precision={naive_metrics[1]:.3f}, recall={naive_metrics[2]:.3f}, f1={naive_metrics[3]:.3f}\")\n",
    "        metrics.append((train_metrics, mode_metrics, naive_metrics))\n",
    "    print(f\"Average train metrics: acc={np.mean([m[0][0] for m in metrics], axis=0):.3f}, precision={np.mean([m[0][1] for m in metrics], axis=0):.3f}, recall={np.mean([m[0][2] for m in metrics], axis=0):.3f}, f1={np.mean([m[0][3] for m in metrics], axis=0):.3f}\")\n",
    "    print(f\"Average mode metrics: acc={np.mean([m[1][0] for m in metrics], axis=0):.3f}, precision={np.mean([m[1][1] for m in metrics], axis=0):.3f}, recall={np.mean([m[1][2] for m in metrics], axis=0):.3f}, f1={np.mean([m[1][3] for m in metrics], axis=0):.3f}\")\n",
    "    print(f\"Average naive metrics: acc={np.mean([m[2][0] for m in metrics], axis=0):.3f}, precision={np.mean([m[2][1] for m in metrics], axis=0):.3f}, recall={np.mean([m[2][2] for m in metrics], axis=0):.3f}, f1={np.mean([m[2][3] for m in metrics], axis=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Train metrics: acc=0.949, precision=0.950, recall=0.949, f1=0.949\n",
      "Mode Strategy metrics: acc=0.790, precision=0.787, recall=0.775, f1=0.774\n",
      "Naive Strategy metrics: acc=0.770, precision=0.750, recall=0.748, f1=0.740\n",
      "Fold 2:\n",
      "Train metrics: acc=0.948, precision=0.949, recall=0.949, f1=0.949\n",
      "Mode Strategy metrics: acc=0.850, precision=0.848, recall=0.837, f1=0.827\n",
      "Naive Strategy metrics: acc=0.800, precision=0.783, recall=0.766, f1=0.770\n",
      "Fold 3:\n",
      "Train metrics: acc=0.949, precision=0.949, recall=0.948, f1=0.948\n",
      "Mode Strategy metrics: acc=0.710, precision=0.692, recall=0.753, f1=0.695\n",
      "Naive Strategy metrics: acc=0.740, precision=0.759, recall=0.757, f1=0.735\n",
      "Fold 4:\n",
      "Train metrics: acc=0.950, precision=0.950, recall=0.950, f1=0.950\n",
      "Mode Strategy metrics: acc=0.780, precision=0.775, recall=0.769, f1=0.758\n",
      "Naive Strategy metrics: acc=0.770, precision=0.761, recall=0.760, f1=0.746\n",
      "Fold 5:\n",
      "Train metrics: acc=0.951, precision=0.951, recall=0.951, f1=0.951\n",
      "Mode Strategy metrics: acc=0.800, precision=0.806, recall=0.815, f1=0.799\n",
      "Naive Strategy metrics: acc=0.780, precision=0.802, recall=0.803, f1=0.795\n",
      "Fold 6:\n",
      "Train metrics: acc=0.948, precision=0.949, recall=0.948, f1=0.948\n",
      "Mode Strategy metrics: acc=0.830, precision=0.821, recall=0.831, f1=0.817\n",
      "Naive Strategy metrics: acc=0.730, precision=0.756, recall=0.733, f1=0.718\n",
      "Fold 7:\n",
      "Train metrics: acc=0.951, precision=0.951, recall=0.951, f1=0.951\n",
      "Mode Strategy metrics: acc=0.740, precision=0.764, recall=0.746, f1=0.741\n",
      "Naive Strategy metrics: acc=0.730, precision=0.766, recall=0.740, f1=0.731\n",
      "Fold 8:\n",
      "Train metrics: acc=0.949, precision=0.949, recall=0.949, f1=0.949\n",
      "Mode Strategy metrics: acc=0.780, precision=0.770, recall=0.779, f1=0.758\n",
      "Naive Strategy metrics: acc=0.780, precision=0.790, recall=0.783, f1=0.767\n",
      "Fold 9:\n",
      "Train metrics: acc=0.953, precision=0.953, recall=0.953, f1=0.953\n",
      "Mode Strategy metrics: acc=0.740, precision=0.744, recall=0.724, f1=0.720\n",
      "Naive Strategy metrics: acc=0.610, precision=0.621, recall=0.587, f1=0.586\n",
      "Fold 10:\n",
      "Train metrics: acc=0.953, precision=0.953, recall=0.953, f1=0.953\n",
      "Mode Strategy metrics: acc=0.828, precision=0.826, recall=0.825, f1=0.818\n",
      "Naive Strategy metrics: acc=0.818, precision=0.805, recall=0.812, f1=0.804\n",
      "Average train metrics: acc=0.950, precision=0.950, recall=0.950, f1=0.950\n",
      "Average mode metrics: acc=0.785, precision=0.783, recall=0.785, f1=0.771\n",
      "Average naive metrics: acc=0.753, precision=0.759, recall=0.749, f1=0.739\n"
     ]
    }
   ],
   "source": [
    "k_fold_experiment(train_folds, test_3sec_folds, test_30sec_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
