{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('../processed_data/split_dataset/data.npz', allow_pickle=True)\n",
    "label = np.load('../processed_data/split_dataset/label.npz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([data[f'arr_{i}'].item()['three_sec'] for i in range(len(data))])\n",
    "X_30sec = np.stack([data[f'arr_{i}'].item()['thirty_sec'] for i in range(len(data))])\n",
    "label = label\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_k_fold_dataset(data_3sec, data_30sec, label, k, shrink_ratio=1.0, shuffle_train_data=True, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    numbers = np.arange(data_3sec.shape[0])\n",
    "    np.random.shuffle(numbers)\n",
    "    test_indices = np.array_split(numbers, k)\n",
    "    \n",
    "    train_folds, test_3sec_folds, test_30sec_folds = [], [], []\n",
    "    for i in range(k):\n",
    "        mask = np.zeros(data_3sec.shape[0], dtype=bool)\n",
    "        mask[test_indices[i]] = True\n",
    "        \n",
    "        X_train, y_train = data_3sec[~mask], label[~mask]\n",
    "        if shuffle_train_data:\n",
    "            shuffle_indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(shuffle_indices)\n",
    "            X_train, y_train = X_train[shuffle_indices], y_train[shuffle_indices]\n",
    "        \n",
    "        X_3sec_test, y_3sec_test = data_3sec[mask], label[mask]\n",
    "        X_30sec_test, y_30sec_test = data_30sec[mask], label[mask]\n",
    "        \n",
    "        train_folds.append((X_train[:int(shrink_ratio * len(X_train))], y_train[:int(shrink_ratio * len(y_train))]))\n",
    "        test_3sec_folds.append((X_3sec_test, y_3sec_test))\n",
    "        test_30sec_folds.append((X_30sec_test, y_30sec_test))\n",
    "    \n",
    "    if shrink_ratio != 1.0:\n",
    "        print(f'Warning: shrink_ratio is set to {shrink_ratio}, the dataset is shrinked.')\n",
    "        print(f'X_train=({train_folds[0][0].shape}, y_train=({train_folds[0][1].shape}), X_3sec_test=({test_30sec_folds[0][0].shape}, y_3sec_test=({test_30sec_folds[0][1].shape})')\n",
    "    return train_folds, test_3sec_folds, test_30sec_folds\n",
    "        \n",
    "train_folds, test_3sec_folds, test_30sec_folds = generate_k_fold_dataset(X, X_30sec, label, 10, shrink_ratio=1.0, shuffle_train_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def predict_and_mode(model, X_test, scaler, threshold=0.6):\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    results = model.predict(X_test_scaled.reshape(-1, X_test.shape[-1]))\n",
    "    results = results.reshape(X_test.shape[:-1])\n",
    "    final_results = stats.mode(results, axis=1)\n",
    "    return final_results.mode.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive(model, X_test, scaler):\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    results = model.predict(X_test_scaled.reshape(-1, X_test.shape[-1]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def aggregate_metrics(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred),\\\n",
    "        precision_score(y_true, y_pred, average='macro', zero_division=0.),\\\n",
    "        recall_score(y_true, y_pred, average='macro', zero_division=0.),\\\n",
    "        f1_score(y_true, y_pred, average='macro', zero_division=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def perform_experiment(train_folds, test_3sec_folds, test_30sec_folds, fold_idx,\n",
    "                       type_return='report'):\n",
    "    \n",
    "    if type_return not in ['report', 'metrics']:\n",
    "        raise ValueError(\"Invalid return_type. Must be either 'report' or 'metrics'.\")\n",
    "    \n",
    "    X_train, y_train = train_folds[fold_idx]\n",
    "    X_test, y_test = test_3sec_folds[fold_idx]\n",
    "    X_test_30sec, y_test_30sec = test_30sec_folds[fold_idx]\n",
    "\n",
    "    rand_perm = np.random.permutation(np.arange(X_test.shape[0]))\n",
    "    X_test, y_test = X_test[rand_perm], y_test[rand_perm]\n",
    "    X_test_30sec, y_test_30sec = X_test_30sec[rand_perm], y_test_30sec[rand_perm]\n",
    "    \n",
    "    y_train = np.repeat(y_train, X_train.shape[1])\n",
    "    X_train = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "    rand_perm = np.random.permutation(np.arange(X_train.shape[0]))\n",
    "    X_train, y_train = X_train[rand_perm], y_train[rand_perm]\n",
    "    \n",
    "    scalar = StandardScaler()\n",
    "    X_train = scalar.fit_transform(X_train)\n",
    "\n",
    "    # Change the model here to experiment with different models\n",
    "    # model = SVC(kernel='rbf', decision_function_shape='ovo', C=1, probability=True, random_state=RANDOM_STATE)\n",
    "    model = MLPClassifier(hidden_layer_sizes=(128, 128, 10), max_iter=1000, random_state=RANDOM_STATE)\n",
    "\n",
    "    # model = RandomForestClassifier(max_features=20, max_depth=10)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if type_return == 'report':\n",
    "        train_report = classification_report(y_train, model.predict(X_train))\n",
    "        mode_report = classification_report(y_test, predict_and_mode(model, X_test, scalar))\n",
    "        naive_report = classification_report(y_test_30sec, predict_naive(model, X_test_30sec, scalar)) \n",
    "        return train_report, mode_report, naive_report\n",
    "    else:\n",
    "        train_metrics = aggregate_metrics(y_train, model.predict(X_train))\n",
    "        mode_metrics = aggregate_metrics(y_test, predict_and_mode(model, X_test, scalar))\n",
    "        naive_metrics = aggregate_metrics(y_test_30sec, predict_naive(model, X_test_30sec, scalar))\n",
    "        return train_metrics, mode_metrics, naive_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_experiment(train_folds, test_3sec_folds, test_30sec_folds):\n",
    "    metrics = []\n",
    "    for k in range(len(train_folds)):\n",
    "        train_metrics, mode_metrics, naive_metrics = perform_experiment(train_folds, test_3sec_folds, test_30sec_folds, k, type_return='metrics')\n",
    "        print(f\"Fold {k+1}:\")\n",
    "        print(f\"Train metrics: acc={train_metrics[0]:.3f}, precision={train_metrics[1]:.3f}, recall={train_metrics[2]:.3f}, f1={train_metrics[3]:.3f}\")\n",
    "        print(f\"Mode Strategy metrics: acc={mode_metrics[0]:.3f}, precision={mode_metrics[1]:.3f}, recall={mode_metrics[2]:.3f}, f1={mode_metrics[3]:.3f}\")\n",
    "        print(f\"Naive Strategy metrics: acc={naive_metrics[0]:.3f}, precision={naive_metrics[1]:.3f}, recall={naive_metrics[2]:.3f}, f1={naive_metrics[3]:.3f}\")\n",
    "        metrics.append((train_metrics, mode_metrics, naive_metrics))\n",
    "    \n",
    "    avg_accs = {\n",
    "        \"train\": np.mean([m[0][0] for m in metrics], axis=0),\n",
    "        \"mode\": np.mean([m[1][0] for m in metrics], axis=0),\n",
    "        \"naive\": np.mean([m[2][0] for m in metrics], axis=0)\n",
    "    }\n",
    "    avg_precisions = {\n",
    "        \"train\": np.mean([m[0][1] for m in metrics], axis=0),\n",
    "        \"mode\": np.mean([m[1][1] for m in metrics], axis=0),\n",
    "        \"naive\": np.mean([m[2][1] for m in metrics], axis=0)\n",
    "    }\n",
    "    avg_recalls = {\n",
    "        \"train\": np.mean([m[0][2] for m in metrics], axis=0),\n",
    "        \"mode\": np.mean([m[1][2] for m in metrics], axis=0),\n",
    "        \"naive\": np.mean([m[2][2] for m in metrics], axis=0)\n",
    "    }\n",
    "    avg_f1s = {\n",
    "        \"train\": np.mean([m[0][3] for m in metrics], axis=0),\n",
    "        \"mode\": np.mean([m[1][3] for m in metrics], axis=0),\n",
    "        \"naive\": np.mean([m[2][3] for m in metrics], axis=0)\n",
    "    }\n",
    "    \n",
    "    print()\n",
    "    print(f\"Average train metrics: acc={avg_accs['train']:.3f},\" +\\\n",
    "          f\"precision={avg_precisions['train']:.3f},\" +\\\n",
    "          f\"recall={avg_recalls['train']:.3f},\" +\\\n",
    "          f\"f1={avg_f1s['train']:.3f}\")\n",
    "    print(f\"Average Mode Strategy metrics: acc={avg_accs['mode']:.3f},\" +\\\n",
    "          f\"precision={avg_precisions['mode']:.3f},\" +\\\n",
    "          f\"recall={avg_recalls['mode']:.3f},\" +\\\n",
    "          f\"f1={avg_f1s['mode']:.3f}\")\n",
    "    print(f\"Average Naive Strategy metrics: acc={avg_accs['naive']:.3f},\" +\\\n",
    "          f\"precision={avg_precisions['naive']:.3f},\" +\\\n",
    "          f\"recall={avg_recalls['naive']:.3f},\" +\\\n",
    "          f\"f1={avg_f1s['naive']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Train metrics: acc=0.999, precision=0.999, recall=0.999, f1=0.999\n",
      "Mode Strategy metrics: acc=0.820, precision=0.793, recall=0.827, f1=0.799\n",
      "Naive Strategy metrics: acc=0.790, precision=0.766, recall=0.784, f1=0.763\n",
      "Fold 2:\n",
      "Train metrics: acc=1.000, precision=1.000, recall=1.000, f1=1.000\n",
      "Mode Strategy metrics: acc=0.780, precision=0.786, recall=0.782, f1=0.777\n",
      "Naive Strategy metrics: acc=0.760, precision=0.732, recall=0.751, f1=0.734\n",
      "Fold 3:\n",
      "Train metrics: acc=0.999, precision=0.999, recall=0.999, f1=0.999\n",
      "Mode Strategy metrics: acc=0.770, precision=0.773, recall=0.788, f1=0.764\n",
      "Naive Strategy metrics: acc=0.780, precision=0.805, recall=0.790, f1=0.790\n",
      "Fold 4:\n",
      "Train metrics: acc=0.999, precision=0.999, recall=0.999, f1=0.999\n",
      "Mode Strategy metrics: acc=0.790, precision=0.781, recall=0.769, f1=0.768\n",
      "Naive Strategy metrics: acc=0.750, precision=0.730, recall=0.731, f1=0.723\n",
      "Fold 5:\n",
      "Train metrics: acc=1.000, precision=1.000, recall=1.000, f1=1.000\n",
      "Mode Strategy metrics: acc=0.760, precision=0.740, recall=0.752, f1=0.729\n",
      "Naive Strategy metrics: acc=0.730, precision=0.701, recall=0.710, f1=0.699\n",
      "Fold 6:\n",
      "Train metrics: acc=0.999, precision=0.999, recall=0.999, f1=0.999\n",
      "Mode Strategy metrics: acc=0.770, precision=0.778, recall=0.776, f1=0.769\n",
      "Naive Strategy metrics: acc=0.740, precision=0.750, recall=0.749, f1=0.738\n",
      "Fold 7:\n",
      "Train metrics: acc=0.999, precision=0.999, recall=0.999, f1=0.999\n",
      "Mode Strategy metrics: acc=0.780, precision=0.757, recall=0.772, f1=0.760\n",
      "Naive Strategy metrics: acc=0.720, precision=0.719, recall=0.715, f1=0.707\n",
      "Fold 8:\n",
      "Train metrics: acc=0.999, precision=0.999, recall=0.999, f1=0.999\n",
      "Mode Strategy metrics: acc=0.730, precision=0.663, recall=0.666, f1=0.660\n",
      "Naive Strategy metrics: acc=0.700, precision=0.653, recall=0.630, f1=0.633\n",
      "Fold 9:\n",
      "Train metrics: acc=0.998, precision=0.998, recall=0.998, f1=0.998\n",
      "Mode Strategy metrics: acc=0.820, precision=0.796, recall=0.782, f1=0.782\n",
      "Naive Strategy metrics: acc=0.860, precision=0.874, recall=0.857, f1=0.860\n",
      "Fold 10:\n",
      "Train metrics: acc=0.999, precision=0.999, recall=0.999, f1=0.999\n",
      "Mode Strategy metrics: acc=0.778, precision=0.783, recall=0.793, f1=0.782\n",
      "Naive Strategy metrics: acc=0.808, precision=0.811, recall=0.820, f1=0.804\n",
      "\n",
      "Average train metrics: acc=0.999,precision=0.999,recall=0.999,f1=0.999\n",
      "Average Mode Strategy metrics: acc=0.780,precision=0.765,recall=0.771,f1=0.759\n",
      "Average Naive Strategy metrics: acc=0.764,precision=0.754,recall=0.754,f1=0.745\n"
     ]
    }
   ],
   "source": [
    "k_fold_experiment(train_folds, test_3sec_folds, test_30sec_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
