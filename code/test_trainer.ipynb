{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\AppData\\Local\\Temp\\ipykernel_18076\\158973850.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00000.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.350088</td>\n",
       "      <td>0.088757</td>\n",
       "      <td>0.130228</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>1784.165850</td>\n",
       "      <td>129774.064525</td>\n",
       "      <td>2002.449060</td>\n",
       "      <td>85882.761315</td>\n",
       "      <td>...</td>\n",
       "      <td>52.420910</td>\n",
       "      <td>-1.690215</td>\n",
       "      <td>36.524071</td>\n",
       "      <td>-0.408979</td>\n",
       "      <td>41.597103</td>\n",
       "      <td>-2.303523</td>\n",
       "      <td>55.062923</td>\n",
       "      <td>1.221291</td>\n",
       "      <td>46.936035</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00001.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.340914</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.095948</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>1530.176679</td>\n",
       "      <td>375850.073649</td>\n",
       "      <td>2039.036516</td>\n",
       "      <td>213843.755497</td>\n",
       "      <td>...</td>\n",
       "      <td>55.356403</td>\n",
       "      <td>-0.731125</td>\n",
       "      <td>60.314529</td>\n",
       "      <td>0.295073</td>\n",
       "      <td>48.120598</td>\n",
       "      <td>-0.283518</td>\n",
       "      <td>51.106190</td>\n",
       "      <td>0.531217</td>\n",
       "      <td>45.786282</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00002.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.363637</td>\n",
       "      <td>0.085275</td>\n",
       "      <td>0.175570</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>1552.811865</td>\n",
       "      <td>156467.643368</td>\n",
       "      <td>1747.702312</td>\n",
       "      <td>76254.192257</td>\n",
       "      <td>...</td>\n",
       "      <td>40.598766</td>\n",
       "      <td>-7.729093</td>\n",
       "      <td>47.639427</td>\n",
       "      <td>-1.816407</td>\n",
       "      <td>52.382141</td>\n",
       "      <td>-3.439720</td>\n",
       "      <td>46.639660</td>\n",
       "      <td>-2.231258</td>\n",
       "      <td>30.573025</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00003.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.404785</td>\n",
       "      <td>0.093999</td>\n",
       "      <td>0.141093</td>\n",
       "      <td>0.006346</td>\n",
       "      <td>1070.106615</td>\n",
       "      <td>184355.942417</td>\n",
       "      <td>1596.412872</td>\n",
       "      <td>166441.494769</td>\n",
       "      <td>...</td>\n",
       "      <td>44.427753</td>\n",
       "      <td>-3.319597</td>\n",
       "      <td>50.206673</td>\n",
       "      <td>0.636965</td>\n",
       "      <td>37.319130</td>\n",
       "      <td>-0.619121</td>\n",
       "      <td>37.259739</td>\n",
       "      <td>-3.407448</td>\n",
       "      <td>31.949339</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00004.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.308526</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.091529</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>1835.004266</td>\n",
       "      <td>343399.939274</td>\n",
       "      <td>1748.172116</td>\n",
       "      <td>88445.209036</td>\n",
       "      <td>...</td>\n",
       "      <td>86.099236</td>\n",
       "      <td>-5.454034</td>\n",
       "      <td>75.269707</td>\n",
       "      <td>-0.916874</td>\n",
       "      <td>53.613918</td>\n",
       "      <td>-4.404827</td>\n",
       "      <td>62.910812</td>\n",
       "      <td>-11.703234</td>\n",
       "      <td>55.195160</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
       "0  blues.00000.wav  661794          0.350088         0.088757  0.130228   \n",
       "1  blues.00001.wav  661794          0.340914         0.094980  0.095948   \n",
       "2  blues.00002.wav  661794          0.363637         0.085275  0.175570   \n",
       "3  blues.00003.wav  661794          0.404785         0.093999  0.141093   \n",
       "4  blues.00004.wav  661794          0.308526         0.087841  0.091529   \n",
       "\n",
       "    rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "0  0.002827             1784.165850          129774.064525   \n",
       "1  0.002373             1530.176679          375850.073649   \n",
       "2  0.002746             1552.811865          156467.643368   \n",
       "3  0.006346             1070.106615          184355.942417   \n",
       "4  0.002303             1835.004266          343399.939274   \n",
       "\n",
       "   spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
       "0              2002.449060            85882.761315  ...   52.420910   \n",
       "1              2039.036516           213843.755497  ...   55.356403   \n",
       "2              1747.702312            76254.192257  ...   40.598766   \n",
       "3              1596.412872           166441.494769  ...   44.427753   \n",
       "4              1748.172116            88445.209036  ...   86.099236   \n",
       "\n",
       "   mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  \\\n",
       "0    -1.690215   36.524071    -0.408979   41.597103    -2.303523   55.062923   \n",
       "1    -0.731125   60.314529     0.295073   48.120598    -0.283518   51.106190   \n",
       "2    -7.729093   47.639427    -1.816407   52.382141    -3.439720   46.639660   \n",
       "3    -3.319597   50.206673     0.636965   37.319130    -0.619121   37.259739   \n",
       "4    -5.454034   75.269707    -0.916874   53.613918    -4.404827   62.910812   \n",
       "\n",
       "   mfcc20_mean  mfcc20_var  label  \n",
       "0     1.221291   46.936035  blues  \n",
       "1     0.531217   45.786282  blues  \n",
       "2    -2.231258   30.573025  blues  \n",
       "3    -3.407448   31.949339  blues  \n",
       "4   -11.703234   55.195160  blues  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "audio_root_folder = './archive/data'\n",
    "labels_csv = os.path.join(audio_root_folder, 'features_30_sec.csv')\n",
    "df = pd.read_csv(labels_csv, header=0)\n",
    "df.drop(df.loc[df.filename == 'jazz.00054.wav'].index, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "RANDOM_SEED = RANDOM_STATE = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.random.manual_seed(RANDOM_SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filenames = df['filename']\n",
    "labels = df['label']\n",
    "\n",
    "files_train, files_val_test, labels_train, labels_val_test = train_test_split(\n",
    "    filenames, labels, test_size=0.1, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "files_val, files_test, labels_val, labels_test = train_test_split(\n",
    "    files_val_test, labels_val_test, test_size=0.5, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audios for Training set: 100%|██████████| 899/899 [00:06<00:00, 130.45it/s]\n",
      "Processing for Training set: 100%|██████████| 899/899 [02:06<00:00,  7.10it/s]\n",
      "Loading audios for Validation set: 100%|██████████| 50/50 [00:00<00:00, 167.14it/s]\n",
      "Processing for Validation set: 100%|██████████| 50/50 [00:06<00:00,  7.25it/s]\n",
      "Loading audios for Testing set: 100%|██████████| 50/50 [00:00<00:00, 151.03it/s]\n",
      "Processing for Testing set: 100%|██████████| 50/50 [00:06<00:00,  7.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from audio_toolbox.dataset import AudioOTFDataset\n",
    "\n",
    "num_frames = 1290\n",
    "label_encoding = 'Label'\n",
    "scaling_strategy = None\n",
    "\n",
    "datasets = {\n",
    "    'train':\n",
    "        AudioOTFDataset(\n",
    "            root_folder=audio_root_folder,\n",
    "            filenames=files_train.tolist(),\n",
    "            labels=labels_train.tolist(),\n",
    "            num_frames=num_frames,\n",
    "            scaling_strategy=scaling_strategy,\n",
    "            name='Training set',\n",
    "            label_encoding=label_encoding,\n",
    "            shuffle=True,\n",
    "            random_state=RANDOM_STATE\n",
    "        ),\n",
    "    'val':\n",
    "        AudioOTFDataset(\n",
    "            root_folder=audio_root_folder,\n",
    "            filenames=files_val.tolist(),\n",
    "            labels=labels_val.tolist(),\n",
    "            num_frames=num_frames,\n",
    "            scaling_strategy=scaling_strategy,\n",
    "            name='Validation set',\n",
    "            label_encoding=label_encoding,\n",
    "            shuffle=True,\n",
    "            random_state=RANDOM_STATE\n",
    "        ),\n",
    "    'test':\n",
    "        AudioOTFDataset(\n",
    "            root_folder=audio_root_folder,\n",
    "            filenames=files_test.tolist(),\n",
    "            labels=labels_test.tolist(),\n",
    "            num_frames=num_frames,\n",
    "            scaling_strategy=scaling_strategy,\n",
    "            name='Testing set',\n",
    "            label_encoding=label_encoding,\n",
    "            shuffle=True,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899, 50, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train, n_val, n_test = len(datasets['train']), len(datasets['val']), len(datasets['test'])\n",
    "n_train, n_val, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training set ======\n",
      "Root folder: ./archive/data\n",
      "Number of samples: 899\n",
      "Shape of one sample: torch.Size([92880])\n",
      "Number of classes: 10\n",
      "Features:\n",
      "\tn_mfcc: 12\n",
      "\tn_chroma: 12\n",
      "\tn_derivatives: 2\n",
      "Scaling strategy: None\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "print(repr(datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved\n",
      "Train label saved\n",
      "Val data saved\n",
      "Val label saved\n",
      "Test data saved\n",
      "Test label saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save the processed tensors to save time next time\n",
    "os.makedirs('./processed_data/dl_data', exist_ok=True)\n",
    "torch.save(datasets['train'].X, 'processed_data/dl_data/dl_modeling_train_data.pt')\n",
    "print('Train data saved')\n",
    "torch.save(datasets['train'].labels, 'processed_data/dl_data/dl_modeling_train_label.pt')\n",
    "print('Train label saved')\n",
    "torch.save(datasets['val'].X, 'processed_data/dl_data/dl_modeling_val_data.pt')\n",
    "print('Val data saved')\n",
    "torch.save(datasets['val'].labels, 'processed_data/dl_data/dl_modeling_val_label.pt')\n",
    "print('Val label saved')\n",
    "torch.save(datasets['test'].X, 'processed_data/dl_data/dl_modeling_test_data.pt')\n",
    "print('Test data saved')\n",
    "torch.save(datasets['test'].labels, 'processed_data/dl_data/dl_modeling_test_label.pt')\n",
    "print('Test label saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "train_scaled = scaler.fit_transform(datasets['train'].X)\n",
    "val_scaled = scaler.transform(datasets['val'].X)\n",
    "test_scaled = scaler.transform(datasets['test'].X)\n",
    "\n",
    "train_data = torch.tensor(pca.fit_transform(train_scaled), dtype=torch.float)\n",
    "val_data = torch.tensor(pca.transform(val_scaled), dtype=torch.float)\n",
    "test_data = torch.tensor(pca.transform(test_scaled), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([899, 753]), torch.Size([50, 753]), torch.Size([50, 753]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved\n",
      "Train label saved\n",
      "Val data saved\n",
      "Val label saved\n",
      "Test data saved\n",
      "Test label saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save the processed tensors to save time next time\n",
    "os.makedirs('./processed_data/dl_data_reduced', exist_ok=True)\n",
    "torch.save(train_data, 'processed_data/dl_data_reduced/dl_modeling_train_data.pt')\n",
    "print('Train data saved')\n",
    "torch.save(datasets['train'].labels, 'processed_data/dl_data_reduced/dl_modeling_train_label.pt')\n",
    "print('Train label saved')\n",
    "torch.save(val_data, 'processed_data/dl_data_reduced/dl_modeling_val_data.pt')\n",
    "print('Val data saved')\n",
    "torch.save(datasets['val'].labels, 'processed_data/dl_data_reduced/dl_modeling_val_label.pt')\n",
    "print('Val label saved')\n",
    "torch.save(test_data, 'processed_data/dl_data_reduced/dl_modeling_test_data.pt')\n",
    "print('Test data saved')\n",
    "torch.save(datasets['test'].labels, 'processed_data/dl_data_reduced/dl_modeling_test_label.pt')\n",
    "print('Test label saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([899, 753]),\n",
       " torch.Size([899]),\n",
       " torch.Size([50, 753]),\n",
       " torch.Size([50]),\n",
       " torch.Size([50, 753]),\n",
       " torch.Size([50]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_data = torch.load('processed_data/dl_data_reduced/dl_modeling_train_data.pt')\n",
    "val_data = torch.load('processed_data/dl_data_reduced/dl_modeling_val_data.pt')\n",
    "test_data = torch.load('processed_data/dl_data_reduced/dl_modeling_test_data.pt')\n",
    "\n",
    "train_label = torch.load('processed_data/dl_data_reduced/dl_modeling_train_label.pt')\n",
    "val_label = torch.load('processed_data/dl_data_reduced/dl_modeling_val_label.pt')\n",
    "test_label = torch.load('processed_data/dl_data_reduced/dl_modeling_test_label.pt')\n",
    "\n",
    "train_data.shape, train_label.shape, val_data.shape, val_label.shape, test_data.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([899, 753])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "datasets = {\n",
    "    'train': TensorDataset(train_data, train_label),\n",
    "    'val': TensorDataset(val_data, val_label),\n",
    "    'test': TensorDataset(test_data, test_label)\n",
    "}\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim=64):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Linear(input_size, hidden_dim),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape (batch_size, input_size)\n",
    "        return self.linear(x.view(x.shape[0], -1))\n",
    "\n",
    "# Example usage:\n",
    "input_size = train_data.size(1)\n",
    "output_size = 10\n",
    "batch_size = 32\n",
    "\n",
    "model = SimpleLinearModel(input_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4  # Adjust the learning rate as needed\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_toolbox.trainer import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer(datasets, model, loss_fn, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    'save': True,\n",
    "    'num_epochs': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 0001, Learning rate: 0.000100, Training loss: 2.29735, Val loss: 2.50260, Test loss: 2.45054, Epoch time: 0.48153\n",
      "INFO:root:Epoch 0002, Learning rate: 0.000100, Training loss: 2.26730, Val loss: 2.26801, Test loss: 2.25943, Epoch time: 0.30904\n",
      "INFO:root:Epoch 0003, Learning rate: 0.000100, Training loss: 2.22474, Val loss: 2.07921, Test loss: 2.10522, Epoch time: 0.30056\n",
      "INFO:root:Epoch 0004, Learning rate: 0.000100, Training loss: 2.18684, Val loss: 1.98731, Test loss: 1.93719, Epoch time: 0.28103\n",
      "INFO:root:Epoch 0005, Learning rate: 0.000100, Training loss: 2.14136, Val loss: 1.82496, Test loss: 1.86597, Epoch time: 0.39868\n",
      "INFO:root:Epoch 0006, Learning rate: 0.000050, Training loss: 2.10889, Val loss: 1.77824, Test loss: 1.77623, Epoch time: 0.32953\n",
      "INFO:root:Epoch 0007, Learning rate: 0.000050, Training loss: 2.09041, Val loss: 1.70111, Test loss: 1.73170, Epoch time: 0.21284\n",
      "INFO:root:Epoch 0008, Learning rate: 0.000050, Training loss: 2.06382, Val loss: 1.65057, Test loss: 1.65046, Epoch time: 0.22143\n",
      "INFO:root:Epoch 0009, Learning rate: 0.000050, Training loss: 2.04312, Val loss: 1.58890, Test loss: 1.55491, Epoch time: 0.27379\n",
      "INFO:root:Epoch 0010, Learning rate: 0.000050, Training loss: 2.02098, Val loss: 1.52929, Test loss: 1.50271, Epoch time: 0.23566\n",
      "INFO:root:Epoch 0011, Learning rate: 0.000025, Training loss: 2.00242, Val loss: 1.45594, Test loss: 1.50098, Epoch time: 0.24409\n",
      "INFO:root:Epoch 0012, Learning rate: 0.000025, Training loss: 1.99563, Val loss: 1.49179, Test loss: 1.49332, Epoch time: 0.23992\n",
      "INFO:root:Epoch 0013, Learning rate: 0.000025, Training loss: 1.97769, Val loss: 1.42638, Test loss: 1.42587, Epoch time: 0.28827\n",
      "INFO:root:Epoch 0014, Learning rate: 0.000025, Training loss: 1.97584, Val loss: 1.40869, Test loss: 1.42536, Epoch time: 0.24559\n",
      "INFO:root:Epoch 0015, Learning rate: 0.000025, Training loss: 1.95453, Val loss: 1.39562, Test loss: 1.39526, Epoch time: 0.25833\n",
      "INFO:root:Epoch 0016, Learning rate: 0.000013, Training loss: 1.95171, Val loss: 1.38172, Test loss: 1.40092, Epoch time: 0.21960\n",
      "INFO:root:Epoch 0017, Learning rate: 0.000013, Training loss: 1.94028, Val loss: 1.37641, Test loss: 1.33196, Epoch time: 0.25499\n",
      "INFO:root:Epoch 0018, Learning rate: 0.000013, Training loss: 1.93306, Val loss: 1.35109, Test loss: 1.45325, Epoch time: 0.23854\n",
      "INFO:root:Epoch 0019, Learning rate: 0.000013, Training loss: 1.94198, Val loss: 1.31914, Test loss: 1.34883, Epoch time: 0.24440\n",
      "INFO:root:Epoch 0020, Learning rate: 0.000013, Training loss: 1.93256, Val loss: 1.32765, Test loss: 1.33862, Epoch time: 0.24116\n",
      "INFO:root:Epoch 0021, Learning rate: 0.000006, Training loss: 1.93280, Val loss: 1.31320, Test loss: 1.29430, Epoch time: 0.27732\n",
      "INFO:root:Epoch 0022, Learning rate: 0.000006, Training loss: 1.93698, Val loss: 1.33872, Test loss: 1.31119, Epoch time: 0.23830\n",
      "INFO:root:Epoch 0023, Learning rate: 0.000006, Training loss: 1.92620, Val loss: 1.33336, Test loss: 1.29598, Epoch time: 0.26273\n",
      "INFO:root:Epoch 0024, Learning rate: 0.000006, Training loss: 1.92910, Val loss: 1.28580, Test loss: 1.34773, Epoch time: 0.23027\n",
      "INFO:root:Epoch 0025, Learning rate: 0.000006, Training loss: 1.93175, Val loss: 1.28538, Test loss: 1.28815, Epoch time: 0.20735\n",
      "INFO:root:Epoch 0026, Learning rate: 0.000003, Training loss: 1.92213, Val loss: 1.35094, Test loss: 1.26226, Epoch time: 0.22929\n",
      "INFO:root:Epoch 0027, Learning rate: 0.000003, Training loss: 1.91192, Val loss: 1.28057, Test loss: 1.27549, Epoch time: 0.23829\n",
      "INFO:root:Epoch 0028, Learning rate: 0.000003, Training loss: 1.90533, Val loss: 1.30524, Test loss: 1.28065, Epoch time: 0.22390\n",
      "INFO:root:Epoch 0029, Learning rate: 0.000003, Training loss: 1.91230, Val loss: 1.27782, Test loss: 1.29645, Epoch time: 0.25919\n",
      "INFO:root:Epoch 0030, Learning rate: 0.000003, Training loss: 1.91594, Val loss: 1.25624, Test loss: 1.30085, Epoch time: 0.22193\n",
      "INFO:root:Epoch 0031, Learning rate: 0.000002, Training loss: 1.90705, Val loss: 1.26735, Test loss: 1.27768, Epoch time: 0.22432\n",
      "INFO:root:Epoch 0032, Learning rate: 0.000002, Training loss: 1.91675, Val loss: 1.31658, Test loss: 1.25015, Epoch time: 0.25144\n",
      "INFO:root:Epoch 0033, Learning rate: 0.000002, Training loss: 1.90884, Val loss: 1.28686, Test loss: 1.26912, Epoch time: 0.19366\n",
      "INFO:root:Epoch 0034, Learning rate: 0.000002, Training loss: 1.91365, Val loss: 1.33096, Test loss: 1.24677, Epoch time: 0.23604\n",
      "INFO:root:Epoch 0035, Learning rate: 0.000002, Training loss: 1.91288, Val loss: 1.24653, Test loss: 1.29242, Epoch time: 0.21704\n",
      "INFO:root:Epoch 0036, Learning rate: 0.000001, Training loss: 1.90858, Val loss: 1.31189, Test loss: 1.28363, Epoch time: 0.22109\n",
      "INFO:root:Epoch 0037, Learning rate: 0.000001, Training loss: 1.90424, Val loss: 1.26096, Test loss: 1.29790, Epoch time: 0.21738\n",
      "INFO:root:Epoch 0038, Learning rate: 0.000001, Training loss: 1.91057, Val loss: 1.27917, Test loss: 1.28194, Epoch time: 0.26796\n",
      "INFO:root:Epoch 0039, Learning rate: 0.000001, Training loss: 1.90059, Val loss: 1.28864, Test loss: 1.31548, Epoch time: 0.21331\n",
      "INFO:root:Epoch 0040, Learning rate: 0.000001, Training loss: 1.90638, Val loss: 1.27522, Test loss: 1.25576, Epoch time: 0.22108\n",
      "INFO:root:Epoch 0041, Learning rate: 0.000000, Training loss: 1.90776, Val loss: 1.29315, Test loss: 1.27693, Epoch time: 0.20796\n",
      "INFO:root:Epoch 0042, Learning rate: 0.000000, Training loss: 1.90527, Val loss: 1.24133, Test loss: 1.30753, Epoch time: 0.22254\n",
      "INFO:root:Epoch 0043, Learning rate: 0.000000, Training loss: 1.91002, Val loss: 1.26176, Test loss: 1.23649, Epoch time: 0.18775\n",
      "INFO:root:Epoch 0044, Learning rate: 0.000000, Training loss: 1.90632, Val loss: 1.27927, Test loss: 1.25944, Epoch time: 0.22616\n",
      "INFO:root:Epoch 0045, Learning rate: 0.000000, Training loss: 1.90663, Val loss: 1.29551, Test loss: 1.27040, Epoch time: 0.22804\n",
      "INFO:root:Epoch 0046, Learning rate: 0.000000, Training loss: 1.91433, Val loss: 1.24114, Test loss: 1.24408, Epoch time: 0.19891\n",
      "INFO:root:Epoch 0047, Learning rate: 0.000000, Training loss: 1.90280, Val loss: 1.27847, Test loss: 1.24396, Epoch time: 0.21865\n",
      "INFO:root:Epoch 0048, Learning rate: 0.000000, Training loss: 1.90037, Val loss: 1.24106, Test loss: 1.35918, Epoch time: 0.21221\n",
      "INFO:root:Epoch 0049, Learning rate: 0.000000, Training loss: 1.90367, Val loss: 1.30888, Test loss: 1.24016, Epoch time: 0.21495\n",
      "INFO:root:Epoch 0050, Learning rate: 0.000000, Training loss: 1.91054, Val loss: 1.27067, Test loss: 1.27793, Epoch time: 0.20691\n",
      "INFO:root:Best model at epoch 0048 with validation loss 1.24106\n",
      "INFO:root:Saving the best model state_dict to my_train_job@2024-03-04-12-54-05\n"
     ]
    }
   ],
   "source": [
    "trainer.train(**trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = trainer.predict(train_data)\n",
    "test_res = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67853170189099"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(train_res, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_res, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
